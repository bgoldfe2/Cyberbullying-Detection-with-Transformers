{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPT5Qv60tIwlIg60nlM9szk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["# Initial Reproduction and Three Extensions of the Cyberbullying Detections Using Transformers Paper\n","### Initial Objective is to recreate\n","  * Attempt to repeat the origin numbers using same hyperparamters\n","  * This will serve as the baseline\n","\n","### Experiment #1 Recreate as an ensemble of binary modules per label\n","  * Have last layer be binary (2) output\n","  * Apply SoftMax layer for probabilities\n","  * Create ensemble with each of the outputs per label\n","  * Compare outputs\n","\n","### Experiment #2 Vertical data augmentation using synthetic data generation\n","  * Leverage GPT-3 for custom data generation per label\n","  * Use as additional data for training\n","  * Compare Outputs\n","\n","### Experiment #3 Horizontal data augmentation using additional label and context content\n","  * Add in additional labels of data serving as additional binary modules for the ensemble\n","  * Add in personal context information to serve as \"normal\" baseline for that individual "],"metadata":{"id":"PjqoB10j7N-V"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gcRbo8UAJ0oA","executionInfo":{"status":"ok","timestamp":1666965866559,"user_tz":0,"elapsed":19089,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"7e4d8f8b-3038-4452-ca9f-f6b6b55cee5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Github/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DiIKv0p_Lsxc","executionInfo":{"status":"ok","timestamp":1666895490056,"user_tz":0,"elapsed":283,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"d3971ac2-e965-493d-94a0-fbdd457d8e12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github\n"]}]},{"cell_type":"code","source":["username = 'bgoldfe2'\n","repository = 'Cyberbullying-Detection-with-Transformers'\n","git_token = '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'\n","\n"],"metadata":{"id":"jbcYyBZPL9YG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://{git_token}@github.com/{username}/{repository}"],"metadata":{"id":"686vmCwbRGU-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd {repository}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jX_tMlrVPIBS","executionInfo":{"status":"ok","timestamp":1666895494316,"user_tz":0,"elapsed":294,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"ebdb8196-101e-462b-e1bf-25d30b7378f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github/Cyberbullying-Detection-with-Transformers\n"]}]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HLxTt7YVPQYX","executionInfo":{"status":"ok","timestamp":1666894492153,"user_tz":0,"elapsed":9820,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"eaa284e6-e177-407e-a221-708ab9d4cea6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n","\u001b[K     |████████████████████████████████| 5.3 MB 5.1 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 73.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (2.23.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 69.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (2022.6.2)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 86.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (4.13.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers->-r requirements.txt (line 1)) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->-r requirements.txt (line 1)) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->-r requirements.txt (line 1)) (3.9.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece\n","Successfully installed huggingface-hub-0.10.1 sentencepiece-0.1.97 tokenizers-0.13.1 transformers-4.23.1\n"]}]},{"cell_type":"code","source":["%cd Scripts/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XFwDF2OHQS8X","executionInfo":{"status":"ok","timestamp":1666895500931,"user_tz":0,"elapsed":617,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"c5df0879-8900-4cda-dc0b-74ea73ef4cbb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github/Cyberbullying-Detection-with-Transformers/Scripts\n"]}]},{"cell_type":"code","source":["!python3 train.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H57OkV8pqhMq","executionInfo":{"status":"ok","timestamp":1666894967599,"user_tz":0,"elapsed":281836,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"51ef6e18-da87-474a-ef35-f9ef10b025ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-10-27 18:18:08.476793: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","47705\n","{'Gender', 'Notcb', 'Others', 'Religion', 'Ethnicity', 'Age'}\n","train len - 28623, valid len - 9541, test len - 9541\n","Downloading: 100% 232k/232k [00:00<00:00, 3.03MB/s]\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 26.7kB/s]\n","Downloading: 100% 570/570 [00:00<00:00, 572kB/s]\n","Downloading: 100% 440M/440M [00:05<00:00, 74.5MB/s]\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","109531974\n","---Starting Training---\n","Epoch 1/1\n","----------\n","100% 1789/1789 [03:22<00:00,  8.81it/s, loss=0.89]\n","Epoch 1 --- Training loss: 0.8902104016603739 Training accuracy: 0.6689\n","100% 299/299 [00:25<00:00, 11.64it/s]\n","Epoch 1 --- Validation loss: 0.5635386041574255 Validation accuracy: 0.8025\n","\n","---History---\n","defaultdict(<class 'list'>, {'train_acc': [0.6689], 'train_loss': [0.8902104016603739], 'val_acc': [0.8025], 'val_loss': [0.5635386041574255]})\n","##################################### Testing ############################################\n","\n","Evaluating: ---bert-base-uncased---\n","\n","100% 299/299 [00:25<00:00, 11.64it/s]\n","Output length --- 9541, Prediction length --- 9541\n","Accuracy: 0.799916151346819\n","Mcc Score: 0.7620330822597359\n","Precision: 0.8053776508330368\n","Recall: 0.799916151346819\n","F1_score: 0.7985574559487204\n","classification_report:                precision    recall  f1-score   support\n","\n","           0     0.8771    0.9612    0.9172      1596\n","           1     0.9545    0.9356    0.9450      1615\n","           2     0.8889    0.7804    0.8311      1630\n","           3     0.5904    0.4653    0.5204      1614\n","           4     0.5723    0.7475    0.6483      1525\n","           5     0.9406    0.9122    0.9262      1561\n","\n","    accuracy                         0.7999      9541\n","   macro avg     0.8040    0.8004    0.7980      9541\n","weighted avg     0.8054    0.7999    0.7986      9541\n","\n","[[1534   18    3   22   14    5]\n"," [  40 1511   10   19   27    8]\n"," [  33   15 1272  183  114   13]\n"," [  54   18   55  751  677   59]\n"," [  24   13   85  258 1140    5]\n"," [  64    8    6   39   20 1424]]\n","##################################### Task End ############################################\n"]}]},{"cell_type":"code","source":["!cat model.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fBUmkY6wtP2y","executionInfo":{"status":"ok","timestamp":1666895507975,"user_tz":0,"elapsed":1541,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"b3f267ab-bf32-414c-9fb9-3bbf28fab077"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["import torch\n","import torch.nn as nn\n","import numpy as np\n","from transformers import BertModel, RobertaModel, XLNetModel, DistilBertModel\n","\n","from common import get_parser\n","\n","parser = get_parser()\n","args = parser.parse_args()\n","np.random.seed(args.seed)\n","torch.manual_seed(args.seed)\n","torch.cuda.manual_seed(args.seed)\n","\n","class BertFGBC(nn.Module):\n","    def __init__(self, pretrained_model = args.pretrained_model):\n","        super().__init__()\n","        self.Bert = BertModel.from_pretrained(pretrained_model)\n","        self.drop1 = nn.Dropout(args.dropout)\n","        self.linear = nn.Linear(args.bert_hidden, 64)\n","        self.batch_norm = nn.LayerNorm(64)\n","        self.drop2 = nn.Dropout(args.dropout)\n","        self.out = nn.Linear(64, args.classes)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        _,last_hidden_state = self.Bert(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            return_dict=False\n","        )\n","        #print(f'Last Hidden State - {last_hidden_state.shape}')\n","        bo = self.drop1(last_hidden_state)\n","        #print(f'Dropout1 - {bo.shape}')\n","        bo = self.linear(bo)\n","        #print(f'Linear1 - {bo.shape}')\n","        bo = self.batch_norm(bo)\n","        #print(f'BatchNorm - {bo.shape}')\n","        bo = nn.Tanh()(bo)\n","        bo = self.drop2(bo)\n","        #print(f'Dropout2 - {bo.shape}')\n","\n","        output = self.out(bo)\n","        #print(f'Output - {output.shape}')\n","        return output\n","\n","class RobertaFGBC(nn.Module):\n","    def __init__(self, pretrained_model = args.pretrained_model):\n","        super().__init__()\n","        self.Roberta = RobertaModel.from_pretrained(pretrained_model)\n","        self.drop1 = nn.Dropout(args.dropout)\n","        self.linear = nn.Linear(args.roberta_hidden, 64)\n","        self.batch_norm = nn.LayerNorm(64)\n","        self.drop2 = nn.Dropout(args.dropout)\n","        self.out = nn.Linear(64, args.classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        _,last_hidden_state = self.Roberta(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            return_dict=False\n","        )\n","\n","        bo = self.drop1(last_hidden_state)\n","        bo = self.linear(bo)\n","        bo = self.batch_norm(bo)\n","        bo = nn.Tanh()(bo)\n","        bo = self.drop2(bo)\n","\n","        output = self.out(bo)\n","\n","        return output\n","\n","class DistilBertFGBC(nn.Module):\n","    def __init__(self, pretrained_model = args.pretrained_model):\n","        super().__init__()\n","        self.DistilBert = DistilBertModel.from_pretrained(pretrained_model)\n","        self.drop1 = nn.Dropout(args.dropout)\n","        self.linear = nn.Linear(args.distilbert_hidden, 64)\n","        self.batch_norm = nn.LayerNorm(64)\n","        self.drop2 = nn.Dropout(args.dropout)\n","        self.out = nn.Linear(64, args.classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        last_hidden_state = self.DistilBert(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            return_dict=False\n","        )\n","\n","        mean_last_hidden_state = self.pool_hidden_state(last_hidden_state)\n","        \n","        bo = self.drop1(mean_last_hidden_state)\n","        bo = self.linear(bo)\n","        bo = self.batch_norm(bo)\n","        bo = nn.Tanh()(bo)\n","        bo = self.drop2(bo)\n","\n","        output = self.out(bo)\n","\n","        return output\n","\n","    def pool_hidden_state(self, last_hidden_state):\n","        last_hidden_state = last_hidden_state[0]\n","        mean_last_hidden_state = torch.mean(last_hidden_state, 1)\n","        return mean_last_hidden_state\n","\n","class XLNetFGBC(nn.Module):\n","    def __init__(self, pretrained_model = args.pretrained_model):\n","        super().__init__()\n","        self.XLNet = XLNetModel.from_pretrained(pretrained_model)\n","        self.drop1 = nn.Dropout(args.dropout)\n","        self.linear = nn.Linear(args.xlnet_hidden, 64)\n","        self.batch_norm = nn.LayerNorm(64)\n","        self.drop2 = nn.Dropout(args.dropout)\n","        self.out = nn.Linear(64, args.classes)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        last_hidden_state = self.XLNet(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            return_dict=False\n","        )\n","        mean_last_hidden_state = self.pool_hidden_state(last_hidden_state)\n","\n","        bo = self.drop1(mean_last_hidden_state)\n","        bo = self.linear(bo)\n","        bo = self.batch_norm(bo)\n","        bo = nn.Tanh()(bo)\n","        bo = self.drop2(bo)\n","\n","        output = self.out(bo)\n","\n","        return output\n","        \n","    def pool_hidden_state(self, last_hidden_state):\n","        last_hidden_state = last_hidden_state[0]\n","        mean_last_hidden_state = torch.mean(last_hidden_state, 1)\n","        return mean_last_hidden_state"]}]}]}