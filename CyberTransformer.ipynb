{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNFFOVoTzuiWFoRWj65Cm7P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["# Initial Reproduction and Three Extensions of the Cyberbullying Detections Using Transformers Paper\n","### Initial Objective is to recreate\n","  * Attempt to repeat the origin numbers using same hyperparamters\n","  * This will serve as the baseline\n","\n","### Experiment #1 Recreate as an ensemble of binary modules per label\n","  * Have last layer be binary (2) output\n","  * Apply SoftMax layer for probabilities\n","  * Create ensemble with each of the outputs per label\n","  * Compare outputs\n","\n","### Experiment #2 Vertical data augmentation using synthetic data generation\n","  * Leverage GPT-3 for custom data generation per label\n","  * Use as additional data for training\n","  * Compare Outputs\n","\n","### Experiment #3 Horizontal data augmentation using additional label and context content\n","  * Add in additional labels of data serving as additional binary modules for the ensemble\n","  * Add in personal context information to serve as \"normal\" baseline for that individual "],"metadata":{"id":"PjqoB10j7N-V"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gcRbo8UAJ0oA","executionInfo":{"status":"ok","timestamp":1669826740051,"user_tz":0,"elapsed":30311,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"945ff61c-5134-4734-fa7c-6de31b748153"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Github/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DiIKv0p_Lsxc","executionInfo":{"status":"ok","timestamp":1669826748843,"user_tz":0,"elapsed":15,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"3a732fd0-e823-49d3-af3c-fa8f4a2962c0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github\n"]}]},{"cell_type":"code","source":["username = 'bgoldfe2'\n","repository = 'Cyberbullying-Detection-with-Transformers'\n","git_token = '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'\n","\n"],"metadata":{"id":"jbcYyBZPL9YG","executionInfo":{"status":"ok","timestamp":1669826821418,"user_tz":0,"elapsed":142,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!git clone https://{git_token}@github.com/{username}/{repository}"],"metadata":{"id":"686vmCwbRGU-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669826823586,"user_tz":0,"elapsed":372,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"2c1c7919-45c6-4171-a490-b9c173a0b9c0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'Cyberbullying-Detection-with-Transformers' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","source":["%cd {repository}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jX_tMlrVPIBS","executionInfo":{"status":"ok","timestamp":1666895494316,"user_tz":0,"elapsed":294,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"ebdb8196-101e-462b-e1bf-25d30b7378f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github/Cyberbullying-Detection-with-Transformers\n"]}]},{"cell_type":"code","source":["!git status"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X81emmSBcCck","executionInfo":{"status":"ok","timestamp":1669826918617,"user_tz":0,"elapsed":18741,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"5fced953-be25-4019-be3a-80aefb10f894"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git checkout -- <file>...\" to discard changes in working directory)\n","\n","\t\u001b[31mmodified:   CyberTransformer.ipynb\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}]},{"cell_type":"code","source":["!git config --global user.email \"bgoldfe2@gmu.edu\"\n","!git config --global user.name \"Bruce Goldfeder\"\n","!git add .\n","!git commit -m \"starting back up nov 30\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UYdD-GwxcPHz","executionInfo":{"status":"ok","timestamp":1669829098498,"user_tz":0,"elapsed":1327,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"765dad02-7000-433e-a6ab-899cd5249b47"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is ahead of 'origin/main' by 1 commit.\n","  (use \"git push\" to publish your local commits)\n","\n","Changes not staged for commit:\n","\t\u001b[31mmodified:   ../CyberTransformer.ipynb\u001b[m\n","\t\u001b[31mmodified:   ../Scripts/model.py\u001b[m\n","\t\u001b[31mmodified:   ../Scripts/train.py\u001b[m\n","\n","no changes added to commit\n"]}]},{"cell_type":"code","source":["!pwd\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NR-Zw7-3kJZJ","executionInfo":{"status":"ok","timestamp":1669829150196,"user_tz":0,"elapsed":171,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"96ebef53-a564-47ef-e320-3c2408273548"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github/Cyberbullying-Detection-with-Transformers\n"]}]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HLxTt7YVPQYX","executionInfo":{"status":"ok","timestamp":1669827089503,"user_tz":0,"elapsed":13860,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"533ed716-388f-4260-d44e-1f2bf9cb4ca3"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 6.7 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 45.5 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (4.13.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 43.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (3.8.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (4.64.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 58.6 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers->-r requirements.txt (line 1)) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->-r requirements.txt (line 1)) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->-r requirements.txt (line 1)) (3.10.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (1.24.3)\n","Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece\n","Successfully installed huggingface-hub-0.11.1 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.24.0\n"]}]},{"cell_type":"code","source":["%cd Scripts/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XFwDF2OHQS8X","executionInfo":{"status":"ok","timestamp":1666895500931,"user_tz":0,"elapsed":617,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"c5df0879-8900-4cda-dc0b-74ea73ef4cbb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github/Cyberbullying-Detection-with-Transformers/Scripts\n"]}]},{"cell_type":"code","source":["%cd Models\n","!ls -alh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wgXbVtlSdPur","executionInfo":{"status":"ok","timestamp":1669827245133,"user_tz":0,"elapsed":411,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"44f19179-2da8-4fa5-c428-471f4fcf715b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github/Cyberbullying-Detection-with-Transformers/Models\n","total 418M\n","-rw------- 1 root root 418M Oct 27 18:22 bert-base-uncased_Best_Val_Acc.bin\n","-rw------- 1 root root    0 Oct 27 16:14 .gitkeep\n"]}]},{"cell_type":"code","source":["# Test run for regression testing\n","!python3 train.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H57OkV8pqhMq","executionInfo":{"status":"ok","timestamp":1666894967599,"user_tz":0,"elapsed":281836,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"51ef6e18-da87-474a-ef35-f9ef10b025ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-10-27 18:18:08.476793: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","47705\n","{'Gender', 'Notcb', 'Others', 'Religion', 'Ethnicity', 'Age'}\n","train len - 28623, valid len - 9541, test len - 9541\n","Downloading: 100% 232k/232k [00:00<00:00, 3.03MB/s]\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 26.7kB/s]\n","Downloading: 100% 570/570 [00:00<00:00, 572kB/s]\n","Downloading: 100% 440M/440M [00:05<00:00, 74.5MB/s]\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","109531974\n","---Starting Training---\n","Epoch 1/1\n","----------\n","100% 1789/1789 [03:22<00:00,  8.81it/s, loss=0.89]\n","Epoch 1 --- Training loss: 0.8902104016603739 Training accuracy: 0.6689\n","100% 299/299 [00:25<00:00, 11.64it/s]\n","Epoch 1 --- Validation loss: 0.5635386041574255 Validation accuracy: 0.8025\n","\n","---History---\n","defaultdict(<class 'list'>, {'train_acc': [0.6689], 'train_loss': [0.8902104016603739], 'val_acc': [0.8025], 'val_loss': [0.5635386041574255]})\n","##################################### Testing ############################################\n","\n","Evaluating: ---bert-base-uncased---\n","\n","100% 299/299 [00:25<00:00, 11.64it/s]\n","Output length --- 9541, Prediction length --- 9541\n","Accuracy: 0.799916151346819\n","Mcc Score: 0.7620330822597359\n","Precision: 0.8053776508330368\n","Recall: 0.799916151346819\n","F1_score: 0.7985574559487204\n","classification_report:                precision    recall  f1-score   support\n","\n","           0     0.8771    0.9612    0.9172      1596\n","           1     0.9545    0.9356    0.9450      1615\n","           2     0.8889    0.7804    0.8311      1630\n","           3     0.5904    0.4653    0.5204      1614\n","           4     0.5723    0.7475    0.6483      1525\n","           5     0.9406    0.9122    0.9262      1561\n","\n","    accuracy                         0.7999      9541\n","   macro avg     0.8040    0.8004    0.7980      9541\n","weighted avg     0.8054    0.7999    0.7986      9541\n","\n","[[1534   18    3   22   14    5]\n"," [  40 1511   10   19   27    8]\n"," [  33   15 1272  183  114   13]\n"," [  54   18   55  751  677   59]\n"," [  24   13   85  258 1140    5]\n"," [  64    8    6   39   20 1424]]\n","##################################### Task End ############################################\n"]}]},{"cell_type":"code","source":["!cat model.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fBUmkY6wtP2y","executionInfo":{"status":"ok","timestamp":1666895507975,"user_tz":0,"elapsed":1541,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"b3f267ab-bf32-414c-9fb9-3bbf28fab077"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["import torch\n","import torch.nn as nn\n","import numpy as np\n","from transformers import BertModel, RobertaModel, XLNetModel, DistilBertModel\n","\n","from common import get_parser\n","\n","parser = get_parser()\n","args = parser.parse_args()\n","np.random.seed(args.seed)\n","torch.manual_seed(args.seed)\n","torch.cuda.manual_seed(args.seed)\n","\n","class BertFGBC(nn.Module):\n","    def __init__(self, pretrained_model = args.pretrained_model):\n","        super().__init__()\n","        self.Bert = BertModel.from_pretrained(pretrained_model)\n","        self.drop1 = nn.Dropout(args.dropout)\n","        self.linear = nn.Linear(args.bert_hidden, 64)\n","        self.batch_norm = nn.LayerNorm(64)\n","        self.drop2 = nn.Dropout(args.dropout)\n","        self.out = nn.Linear(64, args.classes)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        _,last_hidden_state = self.Bert(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            return_dict=False\n","        )\n","        #print(f'Last Hidden State - {last_hidden_state.shape}')\n","        bo = self.drop1(last_hidden_state)\n","        #print(f'Dropout1 - {bo.shape}')\n","        bo = self.linear(bo)\n","        #print(f'Linear1 - {bo.shape}')\n","        bo = self.batch_norm(bo)\n","        #print(f'BatchNorm - {bo.shape}')\n","        bo = nn.Tanh()(bo)\n","        bo = self.drop2(bo)\n","        #print(f'Dropout2 - {bo.shape}')\n","\n","        output = self.out(bo)\n","        #print(f'Output - {output.shape}')\n","        return output\n","\n","class RobertaFGBC(nn.Module):\n","    def __init__(self, pretrained_model = args.pretrained_model):\n","        super().__init__()\n","        self.Roberta = RobertaModel.from_pretrained(pretrained_model)\n","        self.drop1 = nn.Dropout(args.dropout)\n","        self.linear = nn.Linear(args.roberta_hidden, 64)\n","        self.batch_norm = nn.LayerNorm(64)\n","        self.drop2 = nn.Dropout(args.dropout)\n","        self.out = nn.Linear(64, args.classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        _,last_hidden_state = self.Roberta(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            return_dict=False\n","        )\n","\n","        bo = self.drop1(last_hidden_state)\n","        bo = self.linear(bo)\n","        bo = self.batch_norm(bo)\n","        bo = nn.Tanh()(bo)\n","        bo = self.drop2(bo)\n","\n","        output = self.out(bo)\n","\n","        return output\n","\n","class DistilBertFGBC(nn.Module):\n","    def __init__(self, pretrained_model = args.pretrained_model):\n","        super().__init__()\n","        self.DistilBert = DistilBertModel.from_pretrained(pretrained_model)\n","        self.drop1 = nn.Dropout(args.dropout)\n","        self.linear = nn.Linear(args.distilbert_hidden, 64)\n","        self.batch_norm = nn.LayerNorm(64)\n","        self.drop2 = nn.Dropout(args.dropout)\n","        self.out = nn.Linear(64, args.classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        last_hidden_state = self.DistilBert(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            return_dict=False\n","        )\n","\n","        mean_last_hidden_state = self.pool_hidden_state(last_hidden_state)\n","        \n","        bo = self.drop1(mean_last_hidden_state)\n","        bo = self.linear(bo)\n","        bo = self.batch_norm(bo)\n","        bo = nn.Tanh()(bo)\n","        bo = self.drop2(bo)\n","\n","        output = self.out(bo)\n","\n","        return output\n","\n","    def pool_hidden_state(self, last_hidden_state):\n","        last_hidden_state = last_hidden_state[0]\n","        mean_last_hidden_state = torch.mean(last_hidden_state, 1)\n","        return mean_last_hidden_state\n","\n","class XLNetFGBC(nn.Module):\n","    def __init__(self, pretrained_model = args.pretrained_model):\n","        super().__init__()\n","        self.XLNet = XLNetModel.from_pretrained(pretrained_model)\n","        self.drop1 = nn.Dropout(args.dropout)\n","        self.linear = nn.Linear(args.xlnet_hidden, 64)\n","        self.batch_norm = nn.LayerNorm(64)\n","        self.drop2 = nn.Dropout(args.dropout)\n","        self.out = nn.Linear(64, args.classes)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        last_hidden_state = self.XLNet(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            return_dict=False\n","        )\n","        mean_last_hidden_state = self.pool_hidden_state(last_hidden_state)\n","\n","        bo = self.drop1(mean_last_hidden_state)\n","        bo = self.linear(bo)\n","        bo = self.batch_norm(bo)\n","        bo = nn.Tanh()(bo)\n","        bo = self.drop2(bo)\n","\n","        output = self.out(bo)\n","\n","        return output\n","        \n","    def pool_hidden_state(self, last_hidden_state):\n","        last_hidden_state = last_hidden_state[0]\n","        mean_last_hidden_state = torch.mean(last_hidden_state, 1)\n","        return mean_last_hidden_state"]}]}]}