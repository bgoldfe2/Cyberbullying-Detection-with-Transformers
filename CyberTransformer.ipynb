{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMoP/GL2VSPGdDUC2QITA+K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["# Initial Reproduction and Three Extensions of the Cyberbullying Detections Using Transformers Paper\n","### Initial Objective is to recreate\n","  * Attempt to repeat the origin numbers using same hyperparamters\n","  * This will serve as the baseline\n","\n","### Experiment #1 Recreate as an ensemble of binary modules per label\n","  * Have last layer be binary (2) output\n","  * Apply SoftMax layer for probabilities\n","  * Create ensemble with each of the outputs per label\n","  * Compare outputs\n","\n","### Experiment #2 Vertical data augmentation using synthetic data generation\n","  * Leverage GPT-3 for custom data generation per label\n","  * Use as additional data for training\n","  * Compare Outputs\n","\n","### Experiment #3 Horizontal data augmentation using additional label and context content\n","  * Add in additional labels of data serving as additional binary modules for the ensemble\n","  * Add in personal context information to serve as \"normal\" baseline for that individual "],"metadata":{"id":"PjqoB10j7N-V"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gcRbo8UAJ0oA","executionInfo":{"status":"ok","timestamp":1669834678639,"user_tz":0,"elapsed":18607,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"eafe1987-cb30-465e-a796-024a615f4bd4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Github/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DiIKv0p_Lsxc","executionInfo":{"status":"ok","timestamp":1669836817682,"user_tz":0,"elapsed":107,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"e730d48b-7c63-4948-d349-238031850f2f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github\n"]}]},{"cell_type":"code","source":["username = 'bgoldfe2'\n","repository = 'Cyberbullying-Detection-with-Transformers'\n","git_token = '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'\n","\n"],"metadata":{"id":"jbcYyBZPL9YG","executionInfo":{"status":"ok","timestamp":1669836880047,"user_tz":0,"elapsed":109,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!git clone https://{git_token}@github.com/{username}/{repository}"],"metadata":{"id":"686vmCwbRGU-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669826823586,"user_tz":0,"elapsed":372,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"2c1c7919-45c6-4171-a490-b9c173a0b9c0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'Cyberbullying-Detection-with-Transformers' already exists and is not an empty directory.\n"]}]},{"cell_type":"markdown","source":["For working with Git for each session you need to move the Github/ssh folder to /root/ or cd ~ and rename it to .ssh via mv ssh .ssh copy it with the command cp -R /content/drive/MyDrive/Github/Cyberbullying-Detection-with-Transformers/ssh .\n","\n","What a Kludge!!!!"],"metadata":{"id":"97KO9XCo-VC_"}},{"cell_type":"code","source":["%cd {repository}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jX_tMlrVPIBS","executionInfo":{"status":"ok","timestamp":1669836884429,"user_tz":0,"elapsed":111,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"afbe7e49-0a61-4618-e4bd-047d74abe4e7"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github/Cyberbullying-Detection-with-Transformers\n"]}]},{"cell_type":"code","source":["!git status"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X81emmSBcCck","executionInfo":{"status":"ok","timestamp":1669826918617,"user_tz":0,"elapsed":18741,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"5fced953-be25-4019-be3a-80aefb10f894"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git checkout -- <file>...\" to discard changes in working directory)\n","\n","\t\u001b[31mmodified:   CyberTransformer.ipynb\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}]},{"cell_type":"code","source":["!git config --global user.email \"bgoldfe2@gmu.edu\"\n","!git config --global user.name \"Bruce Goldfeder\""],"metadata":{"id":"UYdD-GwxcPHz","executionInfo":{"status":"ok","timestamp":1669836949507,"user_tz":0,"elapsed":460,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HLxTt7YVPQYX","executionInfo":{"status":"ok","timestamp":1669836971504,"user_tz":0,"elapsed":14012,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"657a7493-3d44-4aad-be78-ca503a8867f4"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 5.0 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 46.5 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (4.13.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (3.8.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 38.3 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (1.21.6)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 23.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers->-r requirements.txt (line 1)) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->-r requirements.txt (line 1)) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->-r requirements.txt (line 1)) (3.10.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece\n","Successfully installed huggingface-hub-0.11.1 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.24.0\n"]}]},{"cell_type":"code","source":["%cd Scripts/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XFwDF2OHQS8X","executionInfo":{"status":"ok","timestamp":1669836979935,"user_tz":0,"elapsed":196,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"67b98c46-902e-4e7b-a88d-8f934bed0815"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github/Cyberbullying-Detection-with-Transformers/Scripts\n"]}]},{"cell_type":"code","source":["%cd Models\n","!ls -alh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wgXbVtlSdPur","executionInfo":{"status":"ok","timestamp":1669827245133,"user_tz":0,"elapsed":411,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"44f19179-2da8-4fa5-c428-471f4fcf715b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github/Cyberbullying-Detection-with-Transformers/Models\n","total 418M\n","-rw------- 1 root root 418M Oct 27 18:22 bert-base-uncased_Best_Val_Acc.bin\n","-rw------- 1 root root    0 Oct 27 16:14 .gitkeep\n"]}]},{"cell_type":"code","source":["# Test run for regression testing\n","!python3 train.py --split 'no'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H57OkV8pqhMq","executionInfo":{"status":"ok","timestamp":1669838259566,"user_tz":0,"elapsed":7403,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"89cf2a08-0a88-4966-c673-d3c8d3ea5b09"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["{'Others', 'Gender', 'Religion', 'Ethnicity', 'Age', 'Notcb'}\n","train len - 2126, valid len - 9541, test len - 9541\n","Unnamed: 0\n","text\n","label\n","target\n","train example text --  @NOKIgivesuWINGZ I love my baby @ShoRtTurtLe412 fuck all you dumb niggers hes all I need !!&lt;~I thought u loved my brother @YeaItsLoopy :( \n","with target --  Ethnicity\n","train_dataset object is of type --  <class 'dataset.DatasetBert'>\n","Print Whole object at location 1 --  tensor([  101,  1030,  2053,  3211,  5856,  6961, 25974,  2075,  2480,  1045,\n","         2293,  2026,  3336,  1030,  2460, 20689,  9286, 23632,  2475,  6616,\n","         2035,  2017, 12873,  9152, 13327,  2015,  2002,  2015,  2035,  1045,\n","         2342,   999,   999,  1004,  8318,  1025,  1066,  1045,  2245,  1057,\n","         3866,  2026,  2567,  1030,  6300,  4886,  3215,  4135,  7361,  2100,\n","         1024,  1006,   102,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0])\n","token example is --  ['[CLS]', '@', 'no', '##ki', '##gi', '##ves', '##uw', '##ing', '##z', 'i', 'love', 'my', 'baby', '@', 'short', '##tur', '##tle', '##41', '##2', 'fuck', 'all', 'you', 'dumb', 'ni', '##gger', '##s', 'he', '##s', 'all', 'i', 'need', '!', '!', '&', 'lt', ';', '~', 'i', 'thought', 'u', 'loved', 'my', 'brother', '@', 'ye', '##ai', '##ts', '##lo', '##op', '##y', ':', '(', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","Traceback (most recent call last):\n","  File \"train.py\", line 184, in <module>\n","    run()\n","  File \"train.py\", line 44, in run\n","    asfd\n","NameError: name 'asfd' is not defined\n"]}]},{"cell_type":"code","source":["!cat model.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fBUmkY6wtP2y","executionInfo":{"status":"ok","timestamp":1666895507975,"user_tz":0,"elapsed":1541,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"b3f267ab-bf32-414c-9fb9-3bbf28fab077"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["import torch\n","import torch.nn as nn\n","import numpy as np\n","from transformers import BertModel, RobertaModel, XLNetModel, DistilBertModel\n","\n","from common import get_parser\n","\n","parser = get_parser()\n","args = parser.parse_args()\n","np.random.seed(args.seed)\n","torch.manual_seed(args.seed)\n","torch.cuda.manual_seed(args.seed)\n","\n","class BertFGBC(nn.Module):\n","    def __init__(self, pretrained_model = args.pretrained_model):\n","        super().__init__()\n","        self.Bert = BertModel.from_pretrained(pretrained_model)\n","        self.drop1 = nn.Dropout(args.dropout)\n","        self.linear = nn.Linear(args.bert_hidden, 64)\n","        self.batch_norm = nn.LayerNorm(64)\n","        self.drop2 = nn.Dropout(args.dropout)\n","        self.out = nn.Linear(64, args.classes)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        _,last_hidden_state = self.Bert(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            return_dict=False\n","        )\n","        #print(f'Last Hidden State - {last_hidden_state.shape}')\n","        bo = self.drop1(last_hidden_state)\n","        #print(f'Dropout1 - {bo.shape}')\n","        bo = self.linear(bo)\n","        #print(f'Linear1 - {bo.shape}')\n","        bo = self.batch_norm(bo)\n","        #print(f'BatchNorm - {bo.shape}')\n","        bo = nn.Tanh()(bo)\n","        bo = self.drop2(bo)\n","        #print(f'Dropout2 - {bo.shape}')\n","\n","        output = self.out(bo)\n","        #print(f'Output - {output.shape}')\n","        return output\n","\n","class RobertaFGBC(nn.Module):\n","    def __init__(self, pretrained_model = args.pretrained_model):\n","        super().__init__()\n","        self.Roberta = RobertaModel.from_pretrained(pretrained_model)\n","        self.drop1 = nn.Dropout(args.dropout)\n","        self.linear = nn.Linear(args.roberta_hidden, 64)\n","        self.batch_norm = nn.LayerNorm(64)\n","        self.drop2 = nn.Dropout(args.dropout)\n","        self.out = nn.Linear(64, args.classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        _,last_hidden_state = self.Roberta(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            return_dict=False\n","        )\n","\n","        bo = self.drop1(last_hidden_state)\n","        bo = self.linear(bo)\n","        bo = self.batch_norm(bo)\n","        bo = nn.Tanh()(bo)\n","        bo = self.drop2(bo)\n","\n","        output = self.out(bo)\n","\n","        return output\n","\n","class DistilBertFGBC(nn.Module):\n","    def __init__(self, pretrained_model = args.pretrained_model):\n","        super().__init__()\n","        self.DistilBert = DistilBertModel.from_pretrained(pretrained_model)\n","        self.drop1 = nn.Dropout(args.dropout)\n","        self.linear = nn.Linear(args.distilbert_hidden, 64)\n","        self.batch_norm = nn.LayerNorm(64)\n","        self.drop2 = nn.Dropout(args.dropout)\n","        self.out = nn.Linear(64, args.classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        last_hidden_state = self.DistilBert(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            return_dict=False\n","        )\n","\n","        mean_last_hidden_state = self.pool_hidden_state(last_hidden_state)\n","        \n","        bo = self.drop1(mean_last_hidden_state)\n","        bo = self.linear(bo)\n","        bo = self.batch_norm(bo)\n","        bo = nn.Tanh()(bo)\n","        bo = self.drop2(bo)\n","\n","        output = self.out(bo)\n","\n","        return output\n","\n","    def pool_hidden_state(self, last_hidden_state):\n","        last_hidden_state = last_hidden_state[0]\n","        mean_last_hidden_state = torch.mean(last_hidden_state, 1)\n","        return mean_last_hidden_state\n","\n","class XLNetFGBC(nn.Module):\n","    def __init__(self, pretrained_model = args.pretrained_model):\n","        super().__init__()\n","        self.XLNet = XLNetModel.from_pretrained(pretrained_model)\n","        self.drop1 = nn.Dropout(args.dropout)\n","        self.linear = nn.Linear(args.xlnet_hidden, 64)\n","        self.batch_norm = nn.LayerNorm(64)\n","        self.drop2 = nn.Dropout(args.dropout)\n","        self.out = nn.Linear(64, args.classes)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        last_hidden_state = self.XLNet(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            return_dict=False\n","        )\n","        mean_last_hidden_state = self.pool_hidden_state(last_hidden_state)\n","\n","        bo = self.drop1(mean_last_hidden_state)\n","        bo = self.linear(bo)\n","        bo = self.batch_norm(bo)\n","        bo = nn.Tanh()(bo)\n","        bo = self.drop2(bo)\n","\n","        output = self.out(bo)\n","\n","        return output\n","        \n","    def pool_hidden_state(self, last_hidden_state):\n","        last_hidden_state = last_hidden_state[0]\n","        mean_last_hidden_state = torch.mean(last_hidden_state, 1)\n","        return mean_last_hidden_state"]}]}]}